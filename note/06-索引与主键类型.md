# 主键索引

TiDB 中，关于 Primary Key 的默认定义与 MySQL 常用存储引擎 InnoDB 不一致。InnoDB 中，Primary Key 的语义为：唯一，不为空，且为聚簇索引。

而在 TiDB 中，Primary Key 的定义为：唯一，不为空。但主键不保证为聚簇索引。而是由另一组关键字 CLUSTERED、NONCLUSTERED 额外控制 Primary Key 是否为聚簇索引，若不指定，则由系统变量。索引在TiDB中也是以KV形式存在，其中：

1. 聚簇主键索引：主键列数据（键） - 行数据（值）
2. 非聚簇主键索引：主键列数据（键） - rowid（值），rowid（键）- 行数据（值）。

也就是说：在使用非聚簇主键索引时，TiDB会分配一个隐式的聚簇索引，通过rowid连接主键和行数据。

**注意！！！目前翻阅官方文档时，并未发现任何有关TiDB索引结构的描述，我猜测也是KV类型，并且也是存在一个Region上。**

# 主键类型

## AUTO_INCREMENT

首先，AUTO_INCREMENT的值是PD批量分配给每台 TiDB Server的值（默认 3 万个值），因此能保证唯一性，但分配给 INSERT 语句的值仅在单台 TiDB 服务器上具有单调性。打个比方，初始情况下有3个TiDB Server，分别是S1、S2、S3，所分配的自增值是 S1（1-30000），S2（30001-60000），S3（60001-90000）。

T1时刻，客户端C通过S1插入一条数据，该行主键值=1。

T2时刻，客户端C通过S2插入一条数据，该行主键值=30001。

它和MySQL一样，同时支持隐式分配和显式分配，但TiDB Server的缓存不会因为显式分配过一个特定值而从该特定值从新开始，因此不建议显式分配。

**坑：**但如果客户端在两个TiDB Server上进行Insert操作的话，会导致Primary Key出现跳跃情况（即使通过配置降低缓存大小），而且TiDB Server不会对缓存的自增区间进行持久化，每次重启都会重新向PD拿新的区间，因此频繁重启TiDB Server很可能会导致区间被耗尽。好在TiDB也是支持全局唯一自增的（设置AUTO_ID_CACHE = 1），但是在分布式环境下，这种全局唯一单调自增会带来竞争和同步。

在开启全局唯一自增后，AUTO_INCREMENT值的获取和修改都是在某一个TiDB Server中的一个内存操作，只有当中心化服务的“主” TiDB Server异常崩溃时，才有可能造成少量 ID 不连续，这是因为主备切换时，“备” 节点需要丢弃一部分之前的“主” 节点可能已经分配的 ID，以保证 ID 不出现重复。

**总的来说：比较鸡肋，建议直接用分布式自增ID显式插入，或者兼容旧系统的自增使用。**

## AUTO_RANDOM

**首先，它只能用在BIGINT类型的聚簇索引上**，其次，AUTO_RANDOM(S,R)由2个参数控制，S代表`分片位数`，用来划分`分片区域`，取值范围是[1,15]，默认=5。而R代表`分片区域`的总长度，取值范围是[32,64]，默认=64。S和R分别控制AUTO_RANDOM的`随机性`和`唯一性范围`。

然而，AUTO_RANDOM值结构如下；

| 总位数  | 符号位  |   保留位    | 分片位 |    自增位    |
| :-----: | :-----: | :---------: | :----: | :----------: |
| 64 bits | 0/1 bit | 64 - R bits | S bits | (R-1-S) bits |


可以看到，S与R呈反比例关系，AUTO_RANDOM的计算依赖`当前事务的开始时间的哈希值`确定分片位的值，以及存储引擎中记录的自增位。也就是说：当在R-1的值相等的情况下，S越大随机性越大，但自增范围越小，同时R的值越小。最佳实践是将分片位设置为 log(2, x)，其中 x 为当前集群存储引擎的数量。例如，一个 TiDB 集群中存在 16 个 TiKV，分片位可以设置为 log(2, 16)，即 4。在所有 Region 被均匀调度到各个 TiKV 上以后，此时大批量写入的负载可被均匀分布到不同 TiKV 节点，以实现资源最大化利用。

**总结：可以理解为TIDB自己的分布式自增ID。值得注意的是，作为分片因子S，在确认后不支持动态修改**。

## SHARD_ROW_ID_BITS

略，在使用非聚簇主键时才能设置，依赖row_id，总的来看和AUTO_RANDOM没什么区别。